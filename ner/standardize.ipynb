{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f189563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/mthmy2b53p7ckxhw0pqz94gr0000gn/T/ipykernel_59081/2602423626.py:6: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/Users/dorukyurdusen/Desktop/ai-supported-address-resolution/.pixi/envs/default/lib/python3.12/site-packages/spacy/util.py:922: UserWarning: [W095] Model 'tr_pipeline' (0.0.0) was trained with spaCy v3.4.4 and may not be 100% compatible with the current version (3.8.7). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy,pickle,faiss,re,os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from spacy.tokens import Doc\n",
    "from rapidfuzz import process, fuzz\n",
    "from tqdm.autonotebook import tqdm \n",
    "from collections import Counter  # We'll use Counter for voting\n",
    "\n",
    "\n",
    "columns_to_read = [\"city_name\", \"district_name\", \"quarter_name\", \"street_name\"]\n",
    "column_types = {\n",
    "    \"city_name\": pl.Categorical,\n",
    "    \"district_name\": pl.Categorical,\n",
    "    \"quarter_name\": pl.Categorical\n",
    "}\n",
    "base_df = pl.read_csv(\n",
    "    \"../data/base_df_filtered.csv\",\n",
    "    columns=columns_to_read, \n",
    "    schema_overrides=column_types \n",
    ")\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"./ner-model/ner-model-best\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "#train_df = pl.read_csv(\"../dataset/train.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee014be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved search artifacts...\n",
      "Artifacts loaded successfully.\n",
      "Index Type: <class 'faiss.swigfaiss.IndexIVFPQ'>\n",
      "Total Number of Streets in Index: 71809\n",
      "FAISS search precision (nprobe) set to 10.\n"
     ]
    }
   ],
   "source": [
    "VECTORIZER_PATH = \"vectorizer/street_vectorizer.pkl\"\n",
    "INDEX_PATH = \"vectorizer/street_index_ivfpq.faiss\"\n",
    "STREET_LIST_PATH = \"vectorizer/all_streets_list.pkl\"\n",
    "\n",
    "print(\"Loading saved search artifacts...\")\n",
    "try:\n",
    "    with open(VECTORIZER_PATH, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "\n",
    "    with open(STREET_LIST_PATH, 'rb') as f:\n",
    "        all_streets_list = pickle.load(f)\n",
    "\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    print(\"Artifacts loaded successfully.\")\n",
    "    print(f\"Index Type: {type(index)}\")\n",
    "    print(f\"Total Number of Streets in Index: {index.ntotal}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Required search files not found. Go to vectorizer folder and run vectorize.ipynb\")\n",
    "\n",
    "\n",
    "# --- Search Performance Setting: nprobe ---\n",
    "# This determines how many \"buckets\" (centroids) FAISS will check during search.\n",
    "# The higher the value, the more precise—but also slower—the search becomes.\n",
    "# The lower the value, the faster the search, but there is a small risk of missing the best match.\n",
    "# A value around 1 % of nlist is generally a good starting point.\n",
    "index.nprobe = 10\n",
    "print(f\"FAISS search precision (nprobe) set to {index.nprobe}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c407c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique cities: 5\n",
      "Total unique districts: 96\n",
      "Total unique quarters: 2937\n",
      "Total unique streets: 71809\n"
     ]
    }
   ],
   "source": [
    "ALL_CITIES = set(base_df.get_column(\"city_name\").drop_nulls().unique().to_list())\n",
    "ALL_DISTRICTS = set(base_df.get_column(\"district_name\").drop_nulls().unique().to_list())\n",
    "ALL_QUARTERS = set(base_df.get_column(\"quarter_name\").drop_nulls().unique().to_list())\n",
    "ALL_STREETS = set(base_df.get_column(\"street_name\").drop_nulls().unique().to_list())\n",
    "print(f\"Total unique cities: {len(ALL_CITIES)}\")\n",
    "print(f\"Total unique districts: {len(ALL_DISTRICTS)}\")\n",
    "print(f\"Total unique quarters: {len(ALL_QUARTERS)}\")\n",
    "print(f\"Total unique streets: {len(ALL_STREETS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfed0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_map = str.maketrans(\"ğüşöçıİ\", \"gusocii\")\n",
    "def normalize(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).lower().translate(turkish_map).replace(\"i̇\",\"i\").strip()\n",
    "\n",
    "key_to_column_map = {\n",
    "    'il': 'city_name',\n",
    "    'ilce': 'district_name',\n",
    "    'mahalle': 'quarter_name',\n",
    "    'sokak': 'street_name'\n",
    "}\n",
    "\n",
    "\n",
    "# This pattern targets address keywords at the **end** of a string.\n",
    "# It captures not only the keywords but also the leading whitespace\n",
    "# and any trailing suffixes like 'NDA, 'NE, etc.\n",
    "name_extractor_pattern = re.compile(\n",
    "    r\"\\s+\\b(\"  # Capture the space before the keyword\n",
    "    r\"MAHALLESİ|MAHALLE|MAH|\"\n",
    "    r\"CADDESİ|CADDE|CAD|CD|\"\n",
    "    r\"SOKAĞI|SOKAK|SOK|SK|\"\n",
    "    r\"BULVARI|BULVAR|BLV|\"\n",
    "    r\"MEYDANI|MEYDAN|MEYD|\"\n",
    "    r\"APARTMANI|APT|AP|\"\n",
    "    r\"SİTESİ|SİTE|SİT\"\n",
    "    r\")\\b\\.?(?:'[A-ZİÖÜÇŞĞ]+)?\\s*$\",  # Capture optional dot, suffixes like 'NDA, and trailing spaces\n",
    "    flags=re.IGNORECASE | re.UNICODE\n",
    ")\n",
    "\n",
    "def extract_name_part(text):\n",
    "    \"\"\"\n",
    "    Removes address keywords at the end of a component and\n",
    "    returns only the proper name.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Match the pattern in the text and replace the matched part with a space.\n",
    "    cleaned_text = name_extractor_pattern.sub(\"\", text)\n",
    "    \n",
    "    # Trim any leading/trailing whitespace that may remain.\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def ent2dict(doc: Doc) -> dict:\n",
    "    \"\"\"\n",
    "    Convert entities from a spaCy Doc object into a predefined\n",
    "    dictionary structure.\n",
    "    \n",
    "    Args:\n",
    "        doc (spacy.tokens.Doc): The processed spaCy document.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary populated with the entity text.\n",
    "    \"\"\"\n",
    "    entity_data = {\n",
    "        'il': None,\n",
    "        'ilce': None,\n",
    "        'mahalle': None,\n",
    "        'sokak': None,\n",
    "        'semt': None, \n",
    "        'pk': None,  \n",
    "        'diger': None\n",
    "    }\n",
    "    other_entities = []\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_.lower()\n",
    "        if label in entity_data:\n",
    "            entity_data[label] = extract_name_part(normalize(ent.text))\n",
    "        else:\n",
    "            other_entities.append(f\"{normalize(ent.text)} ({ent.label_})\")\n",
    "    if other_entities:\n",
    "        entity_data['other'] = other_entities\n",
    "        \n",
    "    return entity_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trusted_entity_cache = {}\n",
    "#Caching is not recommended since it takes lots of space when extracting big datasets and causes kernel to crash.\n",
    "def find_trusted_entity_cached(\n",
    "    query: str,\n",
    "    choices: set,\n",
    "    entity_type: str,\n",
    "    trust_threshold: float = 90,\n",
    "    scorer=fuzz.token_set_ratio,\n",
    ") -> str | None:\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    #cache_key = f\"{entity_type}_{query}_{scorer.__name__}\"\n",
    "    # if cache_key in trusted_entity_cache:\n",
    "    #     return trusted_entity_cache[cache_key]\n",
    "\n",
    "    if query in choices:\n",
    "        result = query\n",
    "    else:\n",
    "        extracted_match = process.extractOne(query, choices, scorer=scorer)\n",
    "        result = None\n",
    "        if extracted_match and extracted_match[1] >= trust_threshold:\n",
    "            result = extracted_match[0]\n",
    "\n",
    "    #trusted_entity_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- NEW, MOST ADVANCED AND COMPREHENSIVE Regex Pattern ---\n",
    "# Now also recognizes:\n",
    "# - Slash (/) or hyphen (-) separators\n",
    "# - Possible whitespace (\\s*) around these separators\n",
    "NUMERIC_STREET_PATTERN = re.compile(r'^\\s*\\d+(?:\\s*[/-]\\s*\\d+)?\\s*\\.?\\s*$')\n",
    "\n",
    "\n",
    "\n",
    "# We use vectorizing approach for finding top 50 street match because running \n",
    "# fuzzy on +400000 rows of data every address is really slow.\n",
    "def find_best_street_match(\n",
    "    query_street: str,\n",
    "    k_faiss: int = 50,\n",
    "    score_cutoff: float = 85.0,\n",
    "    scorer=fuzz.token_set_ratio,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds candidates for a street query via FAISS and selects the best one with rapidfuzz.\n",
    "\n",
    "    Args:\n",
    "        query_street (str): The street name to search (e.g., \"ataürk caddesi\").\n",
    "        k_faiss (int): Number of candidates FAISS should retrieve.\n",
    "        score_cutoff (float): Minimum acceptable rapidfuzz score.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_match_str, score) or (None, 0)\n",
    "    \"\"\"\n",
    "    if not query_street or not isinstance(query_street, str):\n",
    "        return None, 0\n",
    "\n",
    "    # --- STAGE 1: FILTER (Fast Candidate Retrieval with FAISS) ---\n",
    "    query_street = normalize(query_street)\n",
    "    query_street_name = extract_name_part(query_street)\n",
    "\n",
    "    # --- UPDATED AND MOST ACCURATE CHECK ---\n",
    "    # Check if the main part of the street matches a numeric format (e.g., \"1778\" or \"1778/7\").\n",
    "    if NUMERIC_STREET_PATTERN.match(query_street_name):\n",
    "        # If matched, remove any trailing dot to return the clean result.\n",
    "        # Example: input \"1778/7.\" returns \"1778/7\".\n",
    "        final_numeric_street = query_street_name.rstrip(\".\")\n",
    "        return final_numeric_street.strip(), 100.0\n",
    "\n",
    "    query_vector = vectorizer.transform([query_street]).toarray().astype(np.float32)\n",
    "\n",
    "    distances, indices = index.search(query_vector, k_faiss)\n",
    "    candidate_streets = [all_streets_list[i] for i in indices[0]]\n",
    "    best_match = process.extractOne(\n",
    "        query_street_name,\n",
    "        candidate_streets,\n",
    "        scorer=scorer,\n",
    "    )\n",
    "\n",
    "    # 5. If the score exceeds our threshold, return the result.\n",
    "    if best_match and best_match[1] >= score_cutoff:\n",
    "        return best_match[0], best_match[1]\n",
    "\n",
    "    # If below threshold or no match found, return None.\n",
    "    return None, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d92a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First version\n",
    "def calculate_match_scores(\n",
    "    df: pl.DataFrame,\n",
    "    entity_dict: dict,\n",
    "    mapping: dict,\n",
    "    score_cutoff: int = 85,\n",
    "    return_top: int = 0,\n",
    "    verbose: int = 0,\n",
    ") -> pl.DataFrame:\n",
    "    \n",
    "    scorers_to_try = [\n",
    "        fuzz.token_set_ratio,\n",
    "        fuzz.ratio,\n",
    "        fuzz.token_sort_ratio,\n",
    "    ]\n",
    "    \n",
    "    filtered_df = pl.DataFrame()\n",
    "    successful_scorer = None\n",
    "    \n",
    "    for scorer in scorers_to_try:\n",
    "        if verbose:\n",
    "            print(f\"--- Trying scorer: {scorer.__name__} ---\")\n",
    "        \n",
    "        found_il = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"il\"), ALL_CITIES, \"il\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        found_ilce = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"ilce\"), ALL_DISTRICTS, \"ilce\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        found_mahalle = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"mahalle\"), ALL_QUARTERS, \"mahalle\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        found_sokak, sokak_score = find_best_street_match(\n",
    "            entity_dict.get(\"sokak\"),\n",
    "            scorer=scorer,\n",
    "            score_cutoff=score_cutoff + 5\n",
    "        )\n",
    "        \n",
    "        temp_df = df\n",
    "        if found_il:\n",
    "            temp_df = temp_df.filter(pl.col(\"city_name\") == found_il)\n",
    "        if found_ilce:\n",
    "            temp_df = temp_df.filter(pl.col(\"district_name\") == found_ilce)\n",
    "        if found_mahalle:\n",
    "            temp_df = temp_df.filter(pl.col(\"quarter_name\") == found_mahalle)\n",
    "        if found_sokak:\n",
    "            temp_df = temp_df.filter(pl.col(\"street_name\") == found_sokak)\n",
    "        \n",
    "        if not temp_df.is_empty():\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Result found with {scorer.__name__}. \"\n",
    "                    f\"Filtered row count: {temp_df.height}\"\n",
    "                )\n",
    "            filtered_df = temp_df\n",
    "            successful_scorer = scorer\n",
    "            break\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"No result found with {scorer.__name__}.\")\n",
    "    \n",
    "    if filtered_df.is_empty():\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"No match found after all attempts. \"\n",
    "                \"Falling back to the NER result.\"\n",
    "            )\n",
    "        fallback_data = {}\n",
    "        # Note: for fallback we use the `found_...` variables from the last attempt.\n",
    "        found_entities = {\n",
    "            \"il\": found_il,\n",
    "            \"ilce\": found_ilce,\n",
    "            \"mahalle\": found_mahalle,\n",
    "            \"sokak\": found_sokak,\n",
    "        }\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            fallback_data[df_col_name] = found_entities.get(dict_key)\n",
    "            fallback_data[f\"{df_col_name}_score\"] = -1.0\n",
    "        fallback_data[\"Overall_Score\"] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "    \n",
    "    score_expressions = []\n",
    "    score_columns_for_avg = []\n",
    "    for dict_key, df_col_name in mapping.items():\n",
    "        text_value = entity_dict.get(dict_key)\n",
    "        score_col_name = f\"{df_col_name}_score\"\n",
    "        score_columns_for_avg.append(score_col_name)\n",
    "        if text_value:\n",
    "            choices = filtered_df.get_column(df_col_name).cast(pl.String).to_list()\n",
    "            scores = process.cdist(\n",
    "                [text_value],\n",
    "                choices,\n",
    "                scorer=successful_scorer,\n",
    "                score_cutoff=0,\n",
    "            )[0]\n",
    "            score_expressions.append(pl.Series(name=score_col_name, values=scores))\n",
    "        else:\n",
    "            score_expressions.append(pl.lit(0).alias(score_col_name))\n",
    "    \n",
    "    final_df = filtered_df.with_columns(score_expressions)\n",
    "    \n",
    "    if score_columns_for_avg:\n",
    "        final_df = final_df.with_columns(\n",
    "            pl.mean_horizontal(score_columns_for_avg).alias(\"Overall_Score\")\n",
    "        )\n",
    "    else:\n",
    "        final_df = final_df.with_columns(pl.lit(0).alias(\"Overall_Score\"))\n",
    "    \n",
    "    final_df = final_df.sort(\"Overall_Score\", descending=True)\n",
    "    \n",
    "    if return_top > 0:\n",
    "        return final_df.head(return_top)\n",
    "    else:\n",
    "        return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe216729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Version\n",
    "def calculate_match_scores_all_scorers(\n",
    "    df: pl.DataFrame,\n",
    "    entity_dict: dict,\n",
    "    mapping: dict,\n",
    "    score_cutoff: int = 85,\n",
    "    return_top: int = 0,\n",
    "    verbose: int = 0,\n",
    ") -> pl.DataFrame:\n",
    "    \n",
    "    scorers_to_try = [\n",
    "        fuzz.token_set_ratio,       \n",
    "        fuzz.ratio, \n",
    "        fuzz.token_sort_ratio,     \n",
    "    ]\n",
    "    \n",
    "    # --- STAGE 1: INFORMATION GATHERING ---\n",
    "    # Collect what each scorer finds for every entity in this dictionary.\n",
    "    found_entities = {\"il\": [], \"ilce\": [], \"mahalle\": [], \"sokak\": []}\n",
    "\n",
    "    for scorer in scorers_to_try:\n",
    "        il = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"il\"), ALL_CITIES, \"il\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        ilce = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"ilce\"), ALL_DISTRICTS, \"ilce\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        mahalle = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"mahalle\"), ALL_QUARTERS, \"mahalle\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        sokak, _ = find_best_street_match(\n",
    "            entity_dict.get(\"sokak\"),\n",
    "            scorer=scorer,\n",
    "            score_cutoff=score_cutoff + 5\n",
    "        )\n",
    "        \n",
    "        if il: found_entities[\"il\"].append(il)\n",
    "        if ilce: found_entities[\"ilce\"].append(ilce)\n",
    "        if mahalle: found_entities[\"mahalle\"].append(mahalle)\n",
    "        if sokak: found_entities[\"sokak\"].append(sokak)\n",
    "    \n",
    "    # --- STAGE 2: INCLUSIVE FILTERING (NEW LOGIC) ---\n",
    "    # Filter using all unique candidates found for each entity.\n",
    "    filtered_df = df\n",
    "    \n",
    "    # Apply filters only for entities present in entity_dict and found by at least one scorer.\n",
    "    for dict_key, df_col_name in mapping.items():\n",
    "        found_list = found_entities.get(dict_key)\n",
    "        \n",
    "        if entity_dict.get(dict_key) and found_list:\n",
    "            # Deduplicate found entities, e.g. ['a', 'b', 'a'] → {'a', 'b'}\n",
    "            unique_entities = list(set(found_list))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Adding to filter -> {df_col_name}.is_in({unique_entities})\")\n",
    "            \n",
    "            # Filter with Polars .is_in() in “OR” logic\n",
    "            filtered_df = filtered_df.filter(pl.col(df_col_name).is_in(unique_entities))\n",
    "            \n",
    "    # --- STAGE 3: SCORING AND FINALIZATION ---\n",
    "    if filtered_df.is_empty():\n",
    "        if verbose:\n",
    "            print(\"No match found after inclusive filtering. Falling back.\")        \n",
    "        fallback_data = {}\n",
    "        # Use the most common result (or any) for fallback.\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            found_list = found_entities.get(dict_key)\n",
    "            fallback_value = Counter(found_list).most_common(1)[0][0] if found_list else None\n",
    "            fallback_data[df_col_name] = fallback_value\n",
    "            fallback_data[f\"{df_col_name}_score\"] = -1.0\n",
    "        fallback_data[\"Overall_Score\"] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "\n",
    "    # Scoring logic (unchanged, most accurate version)\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\nFiltering complete. Calculating final scores \"\n",
    "            f\"over {filtered_df.height} candidate(s)...\"\n",
    "        )\n",
    "    \n",
    "    score_expressions = []\n",
    "    score_columns_for_avg = []\n",
    "    for dict_key, df_col_name in mapping.items():\n",
    "        text_value = entity_dict.get(dict_key)\n",
    "        score_col_name = f\"{df_col_name}_score\"\n",
    "        score_columns_for_avg.append(score_col_name)\n",
    "        if text_value and df_col_name in filtered_df.columns:\n",
    "            choices = filtered_df.get_column(df_col_name).cast(pl.String).to_list()\n",
    "            all_scores = [\n",
    "                process.cdist([text_value], choices, scorer=s)[0]\n",
    "                for s in scorers_to_try\n",
    "            ]\n",
    "            best_scores = np.max(np.array(all_scores), axis=0)\n",
    "            score_expressions.append(\n",
    "                pl.Series(name=score_col_name, values=best_scores)\n",
    "            )\n",
    "        else:\n",
    "            score_expressions.append(pl.lit(0).alias(score_col_name))\n",
    "\n",
    "    final_df = filtered_df.with_columns(score_expressions)\n",
    "\n",
    "    if score_columns_for_avg:\n",
    "        final_df = final_df.with_columns(\n",
    "            pl.mean_horizontal(score_columns_for_avg).alias(\"Overall_Score\")\n",
    "        )\n",
    "    else:\n",
    "        final_df = final_df.with_columns(pl.lit(0).alias(\"Overall_Score\"))\n",
    "        \n",
    "    final_df = final_df.sort(\"Overall_Score\", descending=True)\n",
    "        \n",
    "    return final_df.head(return_top) if return_top > 0 else final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99e97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final version, overall this works better than the others.\n",
    "def calculate_match_scores_hypothesis(\n",
    "    df: pl.DataFrame,\n",
    "    entity_dict: dict,\n",
    "    mapping: dict,\n",
    "    score_cutoff: int = 85,\n",
    "    return_top: int = 0,\n",
    "    verbose: int = 0,\n",
    ") -> pl.DataFrame:\n",
    "    \n",
    "    scorers_to_try = [\n",
    "        fuzz.token_set_ratio,       \n",
    "        fuzz.ratio, \n",
    "        fuzz.token_sort_ratio,     \n",
    "    ]\n",
    "    \n",
    "    # --- STAGE 1: HYPOTHESIS GENERATION ---\n",
    "    hypotheses = []\n",
    "    consolidated_found_entities = {\"il\": [], \"ilce\": [], \"mahalle\": [], \"sokak\": []}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"--- STAGE 1: Generating hypotheses for each scorer... ---\")\n",
    "    for scorer in scorers_to_try:\n",
    "        il = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"il\"), ALL_CITIES, \"il\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        ilce = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"ilce\"), ALL_DISTRICTS, \"ilce\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        mahalle = find_trusted_entity_cached(\n",
    "            entity_dict.get(\"mahalle\"), ALL_QUARTERS, \"mahalle\",\n",
    "            scorer=scorer, trust_threshold=score_cutoff\n",
    "        )\n",
    "        #sokak = find_trusted_entity_cached(entity_dict.get(\"sokak\"), ALL_STREETS, \"sokak\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        sokak, _ = find_best_street_match(\n",
    "            entity_dict.get(\"sokak\"), scorer=scorer, score_cutoff=score_cutoff + 5\n",
    "        )\n",
    "        \n",
    "        hypothesis = {\n",
    "            \"scorer_name\": scorer.__name__,\n",
    "            \"il\": il,\n",
    "            \"ilce\": ilce,\n",
    "            \"mahalle\": mahalle,\n",
    "            \"sokak\": sokak,\n",
    "        }\n",
    "        \n",
    "        if any(v for k, v in hypothesis.items() if k != \"scorer_name\"):\n",
    "            hypotheses.append(hypothesis)\n",
    "            if il: consolidated_found_entities[\"il\"].append(il)\n",
    "            if ilce: consolidated_found_entities[\"ilce\"].append(ilce)\n",
    "            if mahalle: consolidated_found_entities[\"mahalle\"].append(mahalle)\n",
    "            if sokak: consolidated_found_entities[\"sokak\"].append(sokak)\n",
    "            if verbose > 1:\n",
    "                print(\n",
    "                    f\"  -> Hypothesis for {scorer.__name__}: \"\n",
    "                    f\"{ {k: v for k, v in hypothesis.items() if v} }\"\n",
    "                )\n",
    "\n",
    "    if not hypotheses:\n",
    "        if verbose:\n",
    "            print(\"No scorer could generate a meaningful hypothesis. Falling back.\")\n",
    "        # --- FIX 1: INITIAL FALLBACK MECHANISM ---\n",
    "        # To guarantee column order, add names first, then scores.\n",
    "        fallback_data = {}\n",
    "        for col_name in mapping.values():\n",
    "            fallback_data[col_name] = None\n",
    "        for col_name in mapping.values():\n",
    "            fallback_data[f\"{col_name}_score\"] = -1.0\n",
    "        \n",
    "        fallback_data[\"Overall_Score\"] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "            \n",
    "\n",
    "    # --- STAGE 2: EVALUATE EACH HYPOTHESIS SEPARATELY ---\n",
    "    best_result_from_all_hypotheses = pl.DataFrame()\n",
    "    if verbose:\n",
    "        print(\"\\n--- STAGE 2: Testing each hypothesis individually... ---\")\n",
    "    for hypo in hypotheses:\n",
    "        if verbose > 1:\n",
    "            print(f\"-> Testing hypothesis ({hypo['scorer_name']}):\")\n",
    "        \n",
    "        temp_df = df\n",
    "        if hypo.get(\"il\"):\n",
    "            temp_df = temp_df.filter(pl.col(\"city_name\") == hypo[\"il\"])\n",
    "        if hypo.get(\"ilce\"):\n",
    "            temp_df = temp_df.filter(pl.col(\"district_name\") == hypo[\"ilce\"])\n",
    "        if hypo.get(\"mahalle\"):\n",
    "            temp_df = temp_df.filter(pl.col(\"quarter_name\") == hypo[\"mahalle\"])\n",
    "        if hypo.get(\"sokak\"):\n",
    "            temp_df = temp_df.filter(pl.col(\"street_name\") == hypo[\"sokak\"])\n",
    "        \n",
    "        if temp_df.is_empty():\n",
    "            if verbose > 1:\n",
    "                print(\"  No candidate found with this hypothesis.\")\n",
    "            continue\n",
    "            \n",
    "        score_expressions = []\n",
    "        main_score_columns = []\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            text_value = entity_dict.get(dict_key)\n",
    "            score_col_name = f\"{df_col_name}_score\"\n",
    "            main_score_columns.append(score_col_name)\n",
    "            \n",
    "            if text_value and df_col_name in temp_df.columns:\n",
    "                choices = (\n",
    "                    temp_df.get_column(df_col_name).cast(pl.String).to_list()\n",
    "                )\n",
    "                all_scores = [\n",
    "                    process.cdist([text_value], choices, scorer=s)[0]\n",
    "                    for s in scorers_to_try\n",
    "                ]\n",
    "                best_scores = np.max(np.array(all_scores), axis=0)\n",
    "                score_expressions.append(\n",
    "                    pl.Series(name=score_col_name, values=best_scores)\n",
    "                )\n",
    "            else:\n",
    "                score_expressions.append(pl.lit(0).alias(score_col_name))\n",
    "        \n",
    "        hypothesis_result_df = (\n",
    "            temp_df.with_columns(score_expressions)\n",
    "            .with_columns(\n",
    "                pl.mean_horizontal(main_score_columns).alias(\"Overall_Score\")\n",
    "            )\n",
    "            .sort(\"Overall_Score\", descending=True)\n",
    "            .head(1)\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"  -> Best result for {hypo['scorer_name']} hypothesis: \"\n",
    "                f\"{hypothesis_result_df.get_column('Overall_Score')[0]:.2f} score\"\n",
    "            )\n",
    "        \n",
    "        best_result_from_all_hypotheses = pl.concat(\n",
    "            [best_result_from_all_hypotheses, hypothesis_result_df]\n",
    "        )\n",
    "\n",
    "    # --- STAGE 3: SELECTING THE RESULT OF THE BEST HYPOTHESIS ---\n",
    "    if best_result_from_all_hypotheses.is_empty():\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"\\nAll hypotheses tested but no match found in the database. \"\n",
    "                \"Falling back to the best NER guess.\"\n",
    "            )\n",
    "        fallback_data = {}\n",
    "        # Step 1: Fill only the name columns first\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            found_list = consolidated_found_entities.get(dict_key)\n",
    "            fallback_value = (\n",
    "                Counter(found_list).most_common(1)[0][0] if found_list else None\n",
    "            )\n",
    "            fallback_data[df_col_name] = fallback_value\n",
    "        \n",
    "        # Step 2: Then add score columns\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            fallback_data[f\"{df_col_name}_score\"] = -1.0\n",
    "            \n",
    "        fallback_data[\"Overall_Score\"] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "        \n",
    "    if verbose:\n",
    "        print(\n",
    "            \"\\n--- STAGE 3: Selecting the best result among all hypotheses. ---\"\n",
    "        )\n",
    "    \n",
    "    final_df = best_result_from_all_hypotheses.sort(\n",
    "        \"Overall_Score\", descending=True\n",
    "    )\n",
    "        \n",
    "    return final_df.head(return_top) if return_top > 0 else final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c6cbc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>city_name</th><th>district_name</th><th>quarter_name</th><th>street_name</th><th>city_name_score</th><th>district_name_score</th><th>quarter_name_score</th><th>street_name_score</th><th>Overall_Score</th></tr><tr><td>cat</td><td>cat</td><td>cat</td><td>str</td><td>f32</td><td>i32</td><td>i32</td><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mugla&quot;</td><td>&quot;milas&quot;</td><td>&quot;haci ilyas&quot;</td><td>&quot;avcilar&quot;</td><td>100.0</td><td>0</td><td>0</td><td>100.0</td><td>50.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ city_name ┆ district_ ┆ quarter_n ┆ street_na ┆ … ┆ district_ ┆ quarter_n ┆ street_na ┆ Overall_ │\n",
       "│ ---       ┆ name      ┆ ame       ┆ me        ┆   ┆ name_scor ┆ ame_score ┆ me_score  ┆ Score    │\n",
       "│ cat       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ e         ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ cat       ┆ cat       ┆ str       ┆   ┆ ---       ┆ i32       ┆ f32       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ i32       ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ mugla     ┆ milas     ┆ haci      ┆ avcilar   ┆ … ┆ 0         ┆ 0         ┆ 100.0     ┆ 50.0     │\n",
       "│           ┆           ┆ ilyas     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"MUĞLA 48 REZİDANS AVCILAR SOK NO 5  DAİRE  329\"\n",
    "entity_dict = ent2dict(nlp(query))\n",
    "calculate_match_scores_hypothesis(base_df,entity_dict,key_to_column_map,return_top=1,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b90254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>city_name</th><th>district_name</th><th>quarter_name</th><th>street_name</th><th>city_name_score</th><th>district_name_score</th><th>quarter_name_score</th><th>street_name_score</th><th>Overall_Score</th></tr><tr><td>cat</td><td>cat</td><td>cat</td><td>str</td><td>f32</td><td>i32</td><td>i32</td><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mugla&quot;</td><td>&quot;milas&quot;</td><td>&quot;haci ilyas&quot;</td><td>&quot;avcilar&quot;</td><td>100.0</td><td>0</td><td>0</td><td>100.0</td><td>50.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ city_name ┆ district_ ┆ quarter_n ┆ street_na ┆ … ┆ district_ ┆ quarter_n ┆ street_na ┆ Overall_ │\n",
       "│ ---       ┆ name      ┆ ame       ┆ me        ┆   ┆ name_scor ┆ ame_score ┆ me_score  ┆ Score    │\n",
       "│ cat       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ e         ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ cat       ┆ cat       ┆ str       ┆   ┆ ---       ┆ i32       ┆ f32       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ i32       ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ mugla     ┆ milas     ┆ haci      ┆ avcilar   ┆ … ┆ 0         ┆ 0         ┆ 100.0     ┆ 50.0     │\n",
       "│           ┆           ┆ ilyas     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dict = ent2dict(nlp(query))\n",
    "calculate_match_scores(base_df,entity_dict,key_to_column_map,return_top=1,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df94da55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>city_name</th><th>district_name</th><th>quarter_name</th><th>street_name</th><th>city_name_score</th><th>district_name_score</th><th>quarter_name_score</th><th>street_name_score</th><th>Overall_Score</th></tr><tr><td>cat</td><td>cat</td><td>cat</td><td>str</td><td>f32</td><td>i32</td><td>i32</td><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mugla&quot;</td><td>&quot;milas&quot;</td><td>&quot;haci ilyas&quot;</td><td>&quot;avcilar&quot;</td><td>100.0</td><td>0</td><td>0</td><td>100.0</td><td>50.0</td></tr><tr><td>&quot;mugla&quot;</td><td>&quot;mentese&quot;</td><td>&quot;bayir&quot;</td><td>&quot;avcilar&quot;</td><td>100.0</td><td>0</td><td>0</td><td>100.0</td><td>50.0</td></tr><tr><td>&quot;mugla&quot;</td><td>&quot;bodrum&quot;</td><td>&quot;guvercinlik&quot;</td><td>&quot;avcilar&quot;</td><td>100.0</td><td>0</td><td>0</td><td>100.0</td><td>50.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ city_name ┆ district_ ┆ quarter_n ┆ street_na ┆ … ┆ district_ ┆ quarter_n ┆ street_na ┆ Overall_ │\n",
       "│ ---       ┆ name      ┆ ame       ┆ me        ┆   ┆ name_scor ┆ ame_score ┆ me_score  ┆ Score    │\n",
       "│ cat       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ e         ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ cat       ┆ cat       ┆ str       ┆   ┆ ---       ┆ i32       ┆ f32       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ i32       ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ mugla     ┆ milas     ┆ haci      ┆ avcilar   ┆ … ┆ 0         ┆ 0         ┆ 100.0     ┆ 50.0     │\n",
       "│           ┆           ┆ ilyas     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mugla     ┆ mentese   ┆ bayir     ┆ avcilar   ┆ … ┆ 0         ┆ 0         ┆ 100.0     ┆ 50.0     │\n",
       "│ mugla     ┆ bodrum    ┆ guvercinl ┆ avcilar   ┆ … ┆ 0         ┆ 0         ┆ 100.0     ┆ 50.0     │\n",
       "│           ┆           ┆ ik        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dict = ent2dict(nlp(query))\n",
    "calculate_match_scores_all_scorers(base_df,entity_dict,key_to_column_map,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe660496",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_csv(\"../dataset/test.csv\")\n",
    "# --- 1. SETTINGS ---\n",
    "BATCH_SIZE = 50_000\n",
    "OUTPUT_DIRECTORY = \"featured_test_parquet\" \n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "results_list = []\n",
    "batch_counter = 0  # <-- 3. SUGGESTION: Start a clean counter\n",
    "\n",
    "try:\n",
    "    for i, row in enumerate(\n",
    "        tqdm(\n",
    "            test_df.iter_rows(named=True),\n",
    "            total=test_df.height,\n",
    "            desc=\"Processing dataset\"\n",
    "        )\n",
    "    ):\n",
    "        \n",
    "        # --- Main processing logic (unchanged) ---\n",
    "        address_text = row['address']\n",
    "        doc = nlp(address_text)\n",
    "        ent_dict = ent2dict(doc)\n",
    "    \n",
    "        best_match_df = calculate_match_scores_hypothesis(\n",
    "            df=base_df,\n",
    "            entity_dict=ent_dict,\n",
    "            mapping=key_to_column_map,\n",
    "            return_top=1\n",
    "        )\n",
    "        \n",
    "        result_row = row\n",
    "        if not best_match_df.is_empty():\n",
    "            match_dict = best_match_df.to_dicts()[0]\n",
    "            result_row.update(match_dict)\n",
    "        \n",
    "        if 'address' in result_row and isinstance(result_row['address'], str):\n",
    "            result_row['address'] = (\n",
    "                result_row['address'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            )\n",
    "        results_list.append(result_row)\n",
    "        \n",
    "        # --- Batch writing logic ---\n",
    "        # <-- 1. SUGGESTION: Batch condition updated to (i + 1)\n",
    "        if (i + 1) % BATCH_SIZE == 0:\n",
    "            batch_counter += 1  # <-- 3. SUGGESTION: Increment counter\n",
    "            print(f\"\\nWriting batch {batch_counter} to disk...\")\n",
    "            \n",
    "            batch_df = pl.DataFrame(results_list)\n",
    "            output_path = f\"{OUTPUT_DIRECTORY}/batch_{batch_counter}.parquet\"\n",
    "            batch_df.write_parquet(output_path)\n",
    "            \n",
    "            results_list = []  # Reset the list\n",
    "            print(\"Batch written and memory cleared.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProcess interrupted by user. Saving remaining results.\")\n",
    "    # The pass statement allows execution to continue to the finally block\n",
    "\n",
    "finally:\n",
    "    if results_list:\n",
    "        batch_counter += 1 \n",
    "        print(\n",
    "            f\"\\nLoop finished/interrupted. \"\n",
    "            f\"Writing last batch of {len(results_list)} rows ({batch_counter}) to disk...\"\n",
    "        )\n",
    "        \n",
    "        batch_df = pl.DataFrame(results_list)\n",
    "        output_path = f\"{OUTPUT_DIRECTORY}/batch_{batch_counter}.parquet\"\n",
    "        batch_df.write_parquet(output_path)\n",
    "        \n",
    "        results_list = []\n",
    "        print(\"Last batch written and memory cleared.\")\n",
    "\n",
    "    print(\"\\nAll processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
