{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7caa5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load(\"/Users/dorukyurdusen/Desktop/Teknofest/ner/training/new_only/model-best\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "\n",
    "columns_to_read = [\"city_name\", \"district_name\", \"quarter_name\", \"street_name\"]\n",
    "column_types = {\n",
    "    \"city_name\": pl.Categorical,\n",
    "    \"district_name\": pl.Categorical,\n",
    "    \"quarter_name\": pl.Categorical\n",
    "}\n",
    "base_df = pl.read_csv(\n",
    "    \n",
    "    \"/Users/dorukyurdusen/Desktop/Teknofest/dataset/base_df_filtered.csv\",\n",
    "    columns=columns_to_read,  # Sadece bu sütunları diskten oku (ÇOK VERİMLİ)\n",
    "    schema_overrides=column_types      # Okunan sütunların tiplerini anında ayarla\n",
    ")\n",
    "ALL_CITIES = set(base_df.get_column(\"city_name\").drop_nulls().unique().to_list())\n",
    "ALL_DISTRICTS = set(base_df.get_column(\"district_name\").drop_nulls().unique().to_list())\n",
    "ALL_QUARTERS = set(base_df.get_column(\"quarter_name\").drop_nulls().unique().to_list())\n",
    "ALL_STREETS = set(base_df.get_column(\"street_name\").drop_nulls().unique().to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab89a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faz 1: Tek Seferlik Kurulum (IndexIVFPQ ile) Başladı...\n",
      "71809 adet benzersiz sokak adı bulundu.\n",
      "Vektör boyutu: 10000\n",
      "FAISS IndexIVFPQ oluşturuldu. Hücre sayısı (nlist): 1071, Sıkıştırma (m): 16\n",
      "İndeks, verinin bir örneklemi üzerinde eğitiliyor...\n",
      "İndeks eğitildi.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a5217447054362ad19de61a32ebac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tüm Sokaklar İndekse Ekleniyor:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İndekse 71809 adet vektör eklendi.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Faz 1: Tek Seferlik Kurulum (IndexIVFPQ ile) Başladı...\")\n",
    "\n",
    "# --- Adım 1: Veri Yükleme ve Vektörleştirme (Aynı) ---\n",
    "# all_streets_list'in hazır olduğunu varsayıyoruz.\n",
    "all_streets_list = sorted(list(ALL_STREETS))\n",
    "print(f\"{len(all_streets_list)} adet benzersiz sokak adı bulundu.\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 4),\n",
    "    max_features=10000 \n",
    ")\n",
    "vectorizer.fit(all_streets_list)\n",
    "dimension = len(vectorizer.get_feature_names_out())\n",
    "print(f\"Vektör boyutu: {dimension}\")\n",
    "\n",
    "# --- Adım 2: YENİ FAISS İndeksini Tanımlama ---\n",
    "# IVF için hücre (klasör) sayısı. Genel kural: Toplam vektör sayısının karekökünün 4 katı.\n",
    "nlist = int(4 * np.sqrt(len(all_streets_list))) \n",
    "# PQ için alt-vektör sayısı. Boyut (dimension) bu sayıya tam bölünmeli. 16, 32, 64 gibi 2'nin katları idealdir.\n",
    "m = 16\n",
    "# Her alt-vektör için kullanılacak bit sayısı. 8 bit standarttır (2^8=256 kod).\n",
    "nbits = 8 \n",
    "\n",
    "# Önce hücreleri belirlemek için bir \"quantizer\" (niceleyici) oluşturulur.\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Ana IVFPQ indeksini bu quantizer'ı kullanarak oluştur.\n",
    "index = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, nbits)\n",
    "print(f\"FAISS IndexIVFPQ oluşturuldu. Hücre sayısı (nlist): {nlist}, Sıkıştırma (m): {m}\")\n",
    "\n",
    "# --- Adım 3: İndeksi EĞİTME ---\n",
    "# Bu en önemli fark! IVFPQ indeksinin, verinin genel dağılımını öğrenmesi gerekir.\n",
    "# Bunun için tüm veriyi değil, sadece bir örneklemi kullanmak yeterlidir.\n",
    "print(\"İndeks, verinin bir örneklemi üzerinde eğitiliyor...\")\n",
    "# Vektörlerin bir kısmını (örn: 100k) alıp yoğun matrise çevirerek eğitelim.\n",
    "# RAM yetmezse, bu sample_size'ı veya aşağıdaki batch_size'ı düşür.\n",
    "sample_size = min(100000, len(all_streets_list))\n",
    "random_sample_indices = np.random.choice(len(all_streets_list), sample_size, replace=False)\n",
    "sample_streets = [all_streets_list[i] for i in random_sample_indices]\n",
    "\n",
    "vectors_for_training = vectorizer.transform(sample_streets).toarray().astype(np.float32)\n",
    "index.train(vectors_for_training)\n",
    "print(\"İndeks eğitildi.\")\n",
    "\n",
    "# --- Adım 4: Tüm Veriyi Parçalar Halinde İndekse Ekleme (Aynı) ---\n",
    "batch_size = 10000\n",
    "for i in tqdm(range(0, len(all_streets_list), batch_size), desc=\"Tüm Sokaklar İndekse Ekleniyor\"):\n",
    "    batch_streets = all_streets_list[i : i + batch_size]\n",
    "    batch_vectors_sparse = vectorizer.transform(batch_streets)\n",
    "    batch_vectors_dense = batch_vectors_sparse.toarray().astype(np.float32)\n",
    "    index.add(batch_vectors_dense)\n",
    "\n",
    "print(f\"İndekse {index.ntotal} adet vektör eklendi.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad143b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kurulum tamamlandı! 3 varlık da diske kaydedildi:\n",
      "- Vektörleştirici: street_vectorizer.pkl\n",
      "- FAISS İndeksi: street_index_ivfpq.faiss\n",
      "- Sokak Listesi: all_streets_list.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- Adım 5: Varlıkların Diske Kaydedilmesi ---\n",
    "\n",
    "# Kaydedilecek dosya yollarını tanımla\n",
    "vectorizer_path = \"street_vectorizer.pkl\"\n",
    "index_path = \"street_index_ivfpq.faiss\"\n",
    "street_list_path = \"all_streets_list.pkl\"\n",
    "\n",
    "# Vektörleştiriciyi pickle ile kaydet\n",
    "with open(vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Sokak listesini pickle ile kaydet\n",
    "with open(street_list_path, 'wb') as f:\n",
    "    pickle.dump(all_streets_list, f)\n",
    "\n",
    "# FAISS indeksini kendi formatında kaydet (bunu zaten yapmıştın)\n",
    "faiss.write_index(index, index_path)\n",
    "\n",
    "print(f\"\\nKurulum tamamlandı! 3 varlık da diske kaydedildi:\")\n",
    "print(f\"- Vektörleştirici: {vectorizer_path}\")\n",
    "print(f\"- FAISS İndeksi: {index_path}\")\n",
    "print(f\"- Sokak Listesi: {street_list_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44c6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index,\"street_index_ivfpq.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "053e02f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaydedilmiş arama varlıkları yükleniyor...\n",
      "Varlıklar başarıyla yüklendi.\n",
      "İndeks Tipi: <class 'faiss.swigfaiss_avx2.IndexIVFPQ'>\n",
      "İndeksteki Toplam Sokak Sayısı: 71809\n",
      "FAISS arama hassasiyeti (nprobe) 20 olarak ayarlandı.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# --- Gerekli Arama Varlıklarını Belleğe Yükle ---\n",
    "\n",
    "# Kaydettiğimiz dosya yolları\n",
    "VECTORIZER_PATH = \"street_vectorizer.pkl\"\n",
    "INDEX_PATH = \"street_index_ivfpq.faiss\"\n",
    "STREET_LIST_PATH = \"all_streets_list.pkl\"\n",
    "\n",
    "print(\"Kaydedilmiş arama varlıkları yükleniyor...\")\n",
    "try:\n",
    "    with open(VECTORIZER_PATH, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "\n",
    "    with open(STREET_LIST_PATH, 'rb') as f:\n",
    "        all_streets_list = pickle.load(f)\n",
    "\n",
    "    \n",
    "\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    print(\"Varlıklar başarıyla yüklendi.\")\n",
    "    print(f\"İndeks Tipi: {type(index)}\")\n",
    "    print(f\"İndeksteki Toplam Sokak Sayısı: {index.ntotal}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"HATA: Gerekli arama dosyaları bulunamadı. Lütfen önce Faz 1 kurulum script'ini çalıştırın.\")\n",
    "    # Script'i burada durdurmak iyi bir fikir olabilir.\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Arama Performans Ayarı: nprobe ---\n",
    "# Bu, FAISS'in arama yaparken kaç tane \"klasöre\" (centroid'e) bakacağını belirler.\n",
    "# Değer ne kadar yüksekse, arama o kadar hassas ama bir o kadar yavaş olur.\n",
    "# Değer ne kadar düşükse, arama o kadar hızlı ama çok nadir de olsa en iyi sonucu kaçırma riski olur.\n",
    "# Genellikle nlist'in %1'i civarında bir değer iyi bir başlangıçtır.\n",
    "index.nprobe = 20\n",
    "print(f\"FAISS arama hassasiyeti (nprobe) {index.nprobe} olarak ayarlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1893b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process,fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "756170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_map = str.maketrans(\"ğüşöçıİ\", \"gusocii\")\n",
    "def normalize(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).lower().translate(turkish_map).replace(\"i̇\",\"i\").strip()\n",
    "\n",
    "import re\n",
    "\n",
    "# Bu desen, bir metnin sonundaki adres anahtar kelimelerini hedefler.\n",
    "# Sadece kelimeleri değil, başındaki boşluğu ve sonundaki ekleri de ('NDA, 'NE vb.) yakalar.\n",
    "name_extractor_pattern = re.compile(\n",
    "    r\"\\s+\\b(\"  # Anahtar kelimeden önceki boşluğu yakala\n",
    "    r\"MAHALLESİ|MAHALLE|MAH|\"\n",
    "    r\"CADDESİ|CADDE|CAD|CD|\"\n",
    "    r\"SOKAĞI|SOKAK|SOK|SK|\"\n",
    "    r\"BULVARI|BULVAR|BLV|\"\n",
    "    r\"MEYDANI|MEYDAN|MEYD|\"\n",
    "    r\"APARTMANI|APT|AP|\"\n",
    "    r\"SİTESİ|SİTE|SİT\"\n",
    "    r\")\\b\\.?(?:'[A-ZİÖÜÇŞĞ]+)?\\s*$\",  # Sonundaki olası nokta, 'NDA gibi ekler ve boşlukları yakala\n",
    "    flags=re.IGNORECASE | re.UNICODE\n",
    ")\n",
    "\n",
    "def extract_name_part(text):\n",
    "    \"\"\"\n",
    "    Bir adres bileşeninin sonundaki anahtar kelimeleri silerek\n",
    "    sadece özel ismi döndürür.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Deseni metinle eşleştir ve eşleşen kısmı boşlukla değiştir.\n",
    "    cleaned_text = name_extractor_pattern.sub(\"\", text)\n",
    "    \n",
    "    # Sonuçta kalabilecek baş/son boşluklarını temizle.\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2ebbb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yenibagarasi', 96.0, 1008),\n",
       " ('yenibaglar', 78.26086956521739, 507),\n",
       " ('bagarasi', 76.19047619047619, 1244),\n",
       " ('yenibagyaka', 75.0, 2440),\n",
       " ('baglarbasi', 69.56521739130434, 916)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Yeni Bağarası\"\n",
    "query = normalize(extract_name_part(query))\n",
    "process.extract(query,ALL_QUARTERS,scorer=fuzz.ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "634d7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_scorers(query_string: str, choices: list, limit: int = 5):\n",
    "    \"\"\"\n",
    "    Verilen bir sorgu için tüm önemli rapidfuzz scorer'larını dener\n",
    "    ve sonuçları karşılaştırmalı bir Polars DataFrame'i olarak döndürür.\n",
    "    \"\"\"\n",
    "    # 1. Sorguyu hazırla\n",
    "    processed_query = normalize(extract_name_part(query_string))\n",
    "    \n",
    "    # 2. Test edilecek scorer'ların listesi\n",
    "    scorers_to_test = {\n",
    "        \"ratio\": fuzz.ratio,\n",
    "        #\"partial_ratio\": fuzz.partial_ratio,\n",
    "        \"token_sort_ratio\": fuzz.token_sort_ratio,\n",
    "        \"token_set_ratio\": fuzz.token_set_ratio,\n",
    "        \"partial_token_set_ratio\": fuzz.partial_token_set_ratio, # Tavsiyem\n",
    "        \"WRatio\": fuzz.WRatio, # Varsayılan\n",
    "        \"QRatio\": fuzz.QRatio\n",
    "    }\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Sorgu: '{processed_query}'\")\n",
    "    \n",
    "    # 3. Her bir scorer için döngüye gir ve en iyi sonuçları bul\n",
    "    for name, scorer_func in scorers_to_test.items():\n",
    "        # process.extract ile en iyi 'limit' adet sonucu al\n",
    "        best_matches = process.extract(\n",
    "            processed_query,\n",
    "            choices,\n",
    "            scorer=scorer_func,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        # Sonuçları daha sonra DataFrame'e çevirmek üzere formatla\n",
    "        for match, score, index in best_matches:\n",
    "            all_results.append({\n",
    "                \"scorer\": name,\n",
    "                \"match\": match,\n",
    "                \"score\": score\n",
    "            })\n",
    "            \n",
    "    # 4. Tüm sonuçları tek bir Polars DataFrame'ine dönüştür\n",
    "    if not all_results:\n",
    "        return pl.DataFrame({\"scorer\": [], \"match\": [], \"score\": []})\n",
    "        \n",
    "    return pl.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c369c82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yeni Bağarası'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_name_part(nlp(\"Yeni Bağarası mahallesi yeni Foça caddesi no 22/1 1\").ents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7517f67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compare_scorers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compare_df \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_scorers\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myeni bagarasi\u001b[39m\u001b[38;5;124m\"\u001b[39m,ALL_QUARTERS)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compare_scorers' is not defined"
     ]
    }
   ],
   "source": [
    "compare_df = compare_scorers(\"yeni bagarasi\",ALL_QUARTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87ef77cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mehmet kocar', 'mehmet kosar', 'mehmet kocatepe (krc)', 'mehmet kocar/1', 'mehmet korucu', 'kocatepe', 'mehmet kurt', 'mehmet kiyak', 'mehmet kaya', 'kocatepe  4.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mehmet kocatepe (krc)', 100.0, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Mehmet Kocatepe caddesi\"\n",
    "query = extract_name_part(query)\n",
    "query = normalize(query)\n",
    "query_vector = vectorizer.transform([query]).toarray().astype(np.float32)\n",
    "distances, indices = index.search(query_vector, 10)\n",
    "candidate_streets = [all_streets_list[i] for i in indices[0]]\n",
    "print(candidate_streets)\n",
    "process.extractOne(query,candidate_streets,scorer = fuzz.token_set_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8f5753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Mehmet Kocatepe caddesi\"\n",
    "query = extract_name_part(query)\n",
    "query = normalize(query)\n",
    "query_vector = vectorizer.transform([query]).toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05575993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   12,    13,    16,  1418,  1971, 51226, 51617, 72420, 72424,\n",
       "        72425]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = index.search(query_vector, 10)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0809f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71809"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_streets_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trnlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
