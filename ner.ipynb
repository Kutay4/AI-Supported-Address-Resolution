{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f189563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kutaydogan/Documents/GitHub/AI-Supported-Address-Resolution/.pixi/envs/default/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kutaydogan/Documents/GitHub/AI-Supported-Address-Resolution/.pixi/envs/default/lib/python3.12/site-packages/spacy/util.py:922: UserWarning: [W095] Model 'tr_pipeline' (0.0.0) was trained with spaCy v3.4.4 and may not be 100% compatible with the current version (3.8.7). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy,pickle,faiss,re\n",
    "from spacy.tokens import Doc\n",
    "from rapidfuzz import process, fuzz\n",
    "from tqdm.auto import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "columns_to_read = [\"city_name\", \"district_name\", \"quarter_name\", \"street_name\"]\n",
    "column_types = {\n",
    "    \"city_name\": pl.Categorical,\n",
    "    \"district_name\": pl.Categorical,\n",
    "    \"quarter_name\": pl.Categorical\n",
    "}\n",
    "\n",
    "base_df = pl.read_csv(\n",
    "    \"./data/base_df_filtered.csv\",\n",
    "    columns=columns_to_read,  # Sadece bu sütunları diskten oku (ÇOK VERİMLİ)\n",
    "    schema_overrides=column_types      # Okunan sütunların tiplerini anında ayarla\n",
    ")\n",
    "\n",
    "nlp = spacy.load(\"./ner-model/ner-model-best\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "train_df = pl.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee014be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaydedilmiş arama varlıkları yükleniyor...\n",
      "Varlıklar başarıyla yüklendi.\n",
      "İndeks Tipi: <class 'faiss.swigfaiss.IndexIVFPQ'>\n",
      "İndeksteki Toplam Sokak Sayısı: 71809\n",
      "FAISS arama hassasiyeti (nprobe) 10 olarak ayarlandı.\n"
     ]
    }
   ],
   "source": [
    "VECTORIZER_PATH = \"vectorizer/street_vectorizer.pkl\"\n",
    "INDEX_PATH = \"vectorizer/street_index_ivfpq.faiss\"\n",
    "STREET_LIST_PATH = \"vectorizer/all_streets_list.pkl\"\n",
    "\n",
    "print(\"Kaydedilmiş arama varlıkları yükleniyor...\")\n",
    "try:\n",
    "    with open(VECTORIZER_PATH, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "\n",
    "    with open(STREET_LIST_PATH, 'rb') as f:\n",
    "        all_streets_list = pickle.load(f)\n",
    "\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    print(\"Varlıklar başarıyla yüklendi.\")\n",
    "    print(f\"İndeks Tipi: {type(index)}\")\n",
    "    print(f\"İndeksteki Toplam Sokak Sayısı: {index.ntotal}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"HATA: Gerekli arama dosyaları bulunamadı. Lütfen önce Faz 1 kurulum script'ini çalıştırın.\")\n",
    "    # Script'i burada durdurmak iyi bir fikir olabilir.\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Arama Performans Ayarı: nprobe ---\n",
    "# Bu, FAISS'in arama yaparken kaç tane \"klasöre\" (centroid'e) bakacağını belirler.\n",
    "# Değer ne kadar yüksekse, arama o kadar hassas ama bir o kadar yavaş olur.\n",
    "# Değer ne kadar düşükse, arama o kadar hızlı ama çok nadir de olsa en iyi sonucu kaçırma riski olur.\n",
    "# Genellikle nlist'in %1'i civarında bir değer iyi bir başlangıçtır.\n",
    "index.nprobe = 10 \n",
    "print(f\"FAISS arama hassasiyeti (nprobe) {index.nprobe} olarak ayarlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c407c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CITIES = set(base_df.get_column(\"city_name\").drop_nulls().unique().to_list())\n",
    "ALL_DISTRICTS = set(base_df.get_column(\"district_name\").drop_nulls().unique().to_list())\n",
    "ALL_QUARTERS = set(base_df.get_column(\"quarter_name\").drop_nulls().unique().to_list())\n",
    "ALL_STREETS = set(base_df.get_column(\"street_name\").drop_nulls().unique().to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfed0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_map = str.maketrans(\"ğüşöçıİ\", \"gusocii\")\n",
    "def normalize(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).lower().translate(turkish_map).replace(\"i̇\",\"i\").strip()\n",
    "\n",
    "key_to_column_map = {\n",
    "    'il': 'city_name',\n",
    "    'ilce': 'district_name',\n",
    "    'mahalle': 'quarter_name',\n",
    "    'sokak': 'street_name'\n",
    "}\n",
    "\n",
    "\n",
    "# Bu desen, bir metnin sonundaki adres anahtar kelimelerini hedefler.\n",
    "# Sadece kelimeleri değil, başındaki boşluğu ve sonundaki ekleri de ('NDA, 'NE vb.) yakalar.\n",
    "name_extractor_pattern = re.compile(\n",
    "    r\"\\s+\\b(\"  # Anahtar kelimeden önceki boşluğu yakala\n",
    "    r\"MAHALLESİ|MAHALLE|MAH|\"\n",
    "    r\"CADDESİ|CADDE|CAD|CD|\"\n",
    "    r\"SOKAĞI|SOKAK|SOK|SK|\"\n",
    "    r\"BULVARI|BULVAR|BLV|\"\n",
    "    r\"MEYDANI|MEYDAN|MEYD|\"\n",
    "    r\"APARTMANI|APT|AP|\"\n",
    "    r\"SİTESİ|SİTE|SİT\"\n",
    "    r\")\\b\\.?(?:'[A-ZİÖÜÇŞĞ]+)?\\s*$\",  # Sonundaki olası nokta, 'NDA gibi ekler ve boşlukları yakala\n",
    "    flags=re.IGNORECASE | re.UNICODE\n",
    ")\n",
    "\n",
    "def extract_name_part(text):\n",
    "    \"\"\"\n",
    "    Bir adres bileşeninin sonundaki anahtar kelimeleri silerek\n",
    "    sadece özel ismi döndürür.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Deseni metinle eşleştir ve eşleşen kısmı boşlukla değiştir.\n",
    "    cleaned_text = name_extractor_pattern.sub(\"\", text)\n",
    "    \n",
    "    # Sonuçta kalabilecek baş/son boşluklarını temizle.\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def ent2dict(doc: Doc) -> dict:\n",
    "    \"\"\"\n",
    "    Converts entities from a spaCy Doc object into a\n",
    "    predefined dictionary structure.\n",
    "    \n",
    "    Args:\n",
    "        doc (spacy.tokens.Doc): The processed document object from spaCy.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary populated with the entity text.\n",
    "    \"\"\"\n",
    "    entity_data = {\n",
    "    'il': None,\n",
    "    'ilce': None,\n",
    "    'mahalle': None,\n",
    "    'sokak': None,\n",
    "    'semt': None, \n",
    "    'pk': None,  \n",
    "    'diger': None\n",
    "}\n",
    "    other_entities = []\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_.lower()\n",
    "        if label in entity_data:\n",
    "            entity_data[label] = extract_name_part(normalize(ent.text))\n",
    "        else:\n",
    "            other_entities.append(f\"{normalize(ent.text)} ({ent.label_})\")\n",
    "    if other_entities:\n",
    "        entity_data['other'] = other_entities\n",
    "        \n",
    "    return entity_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trusted_entity_cache = {}\n",
    "def find_trusted_entity_cached(query: str, choices: set, entity_type: str, trust_threshold: float = 90, scorer = fuzz.token_set_ratio) -> str | None:\n",
    "    if not query:\n",
    "        return None\n",
    "    \n",
    "    #cache_key = f\"{entity_type}_{query}_{scorer.__name__}\"\n",
    "    # if cache_key in trusted_entity_cache:\n",
    "    #     return trusted_entity_cache[cache_key]\n",
    "    \n",
    "    if query in choices:\n",
    "        result = query\n",
    "    else:\n",
    "        extracted_match = process.extractOne(query, choices, scorer=scorer)\n",
    "        result = None\n",
    "        if extracted_match and extracted_match[1] >= trust_threshold:\n",
    "            result = extracted_match[0]\n",
    "\n",
    "    #trusted_entity_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "# --- YENİ, EN GELİŞMİŞ ve EN KAPSAYICI Regex Deseni ---\n",
    "# Artık şunları da tanır: \n",
    "# - Bölü (/) veya tire (-) işaretleri\n",
    "# - Bu işaretlerin etrafındaki olası boşluklar (\\s*)\n",
    "NUMERIC_STREET_PATTERN = re.compile(r'^\\s*\\d+(?:\\s*[/-]\\s*\\d+)?\\s*\\.?\\s*$')\n",
    "def find_best_street_match(query_street: str, k_faiss: int = 50, score_cutoff: float = 85.0, scorer = fuzz.token_set_ratio):\n",
    "    \"\"\"\n",
    "    Bir sokak sorgusu için FAISS ile adayları bulur ve rapidfuzz ile en iyisini seçer.\n",
    "\n",
    "    Args:\n",
    "        query_street (str): Aranan sokak adı (örn: \"ataürk caddesi\").\n",
    "        k_faiss (int): FAISS'in bulacağı aday sayısı.\n",
    "        score_cutoff (float): Kabul edilebilir minimum rapidfuzz skoru.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (en_iyi_eşleşme_str, skor) veya (None, 0)\n",
    "    \"\"\"\n",
    "    if not query_street or not isinstance(query_street, str):\n",
    "        return None, 0\n",
    "\n",
    "    # --- AŞAMA 1: FİLTRELE (FAISS ile Hızlı Aday Bulma) ---\n",
    "    query_street = normalize(query_street)\n",
    "    query_street_name = extract_name_part(query_street)\n",
    "    \n",
    "     # --- GÜNCELLENMİŞ VE EN DOĞRU KONTROL ---\n",
    "    # Sokağın ana kısmının sayısal formata (örn: \"1778\" veya \"1778/7\") uyup uymadığını kontrol et.\n",
    "    if NUMERIC_STREET_PATTERN.match(query_street_name):\n",
    "        # Eşleşme durumunda, sondaki olası noktayı temizleyerek net sonucu al.\n",
    "        # Örnek: \"1778/7.\" girdisi \"1778/7\" olarak döner.\n",
    "        final_numeric_street = query_street_name.rstrip('.')\n",
    "        return final_numeric_street.strip(), 100.0\n",
    "\n",
    "    query_vector = vectorizer.transform([query_street]).toarray().astype(np.float32)\n",
    "    \n",
    "    distances, indices = index.search(query_vector, k_faiss)\n",
    "    candidate_streets = [all_streets_list[i] for i in indices[0]]\n",
    "    best_match = process.extractOne(\n",
    "        query_street_name,\n",
    "        candidate_streets,\n",
    "        scorer=scorer\n",
    "    )\n",
    "    \n",
    "    # 5. Eğer skor, belirlediğimiz eşik değerinin üzerindeyse sonucu döndür.\n",
    "    if best_match and best_match[1] >= score_cutoff:\n",
    "        return best_match[0], best_match[1]\n",
    "    \n",
    "    # Eşik değerinin altında kalırsa veya eşleşme bulunamazsa None döndür.\n",
    "    return None, 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_scores(df: pl.DataFrame,\n",
    "                                 entity_dict: dict,\n",
    "                                 mapping: dict,\n",
    "                                 score_cutoff = 85,\n",
    "                                 return_top: int = 0,\n",
    "                                 verbose: int = 0,\n",
    "                                 ) -> pl.DataFrame:\n",
    "    \n",
    "    scorers_to_try = [\n",
    "        fuzz.token_set_ratio,       \n",
    "        fuzz.ratio, \n",
    "        fuzz.token_sort_ratio,     \n",
    "    ]\n",
    "    \n",
    "    filtered_df = pl.DataFrame()\n",
    "    successful_scorer = None\n",
    "    \n",
    "    for scorer in scorers_to_try:\n",
    "        if verbose: print(f\"--- Denenen Scorer: {scorer.__name__} ---\")\n",
    "        \n",
    "        found_il = find_trusted_entity_cached(entity_dict.get(\"il\"), ALL_CITIES, \"il\", scorer=scorer,trust_threshold=score_cutoff)\n",
    "        found_ilce = find_trusted_entity_cached(entity_dict.get(\"ilce\"), ALL_DISTRICTS, \"ilce\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        found_mahalle = find_trusted_entity_cached(entity_dict.get(\"mahalle\"), ALL_QUARTERS, \"mahalle\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        found_sokak,sokak_score = find_best_street_match(entity_dict.get(\"sokak\"), scorer=scorer, score_cutoff=score_cutoff+5)\n",
    "\n",
    "        \n",
    "        temp_df = df\n",
    "        if found_il: temp_df = temp_df.filter(pl.col(\"city_name\") == found_il)\n",
    "        if found_ilce: temp_df = temp_df.filter(pl.col(\"district_name\") == found_ilce)\n",
    "        if found_mahalle: temp_df = temp_df.filter(pl.col(\"quarter_name\") == found_mahalle)\n",
    "        if found_sokak: temp_df = temp_df.filter(pl.col(\"street_name\") == found_sokak)\n",
    "        \n",
    "        if not temp_df.is_empty():\n",
    "            if verbose: print(f\"Sonuç, {scorer.__name__} ile bulundu. Filtrelenmiş satır sayısı: {temp_df.height}\")\n",
    "            filtered_df = temp_df \n",
    "            successful_scorer = scorer \n",
    "            break \n",
    "        else:\n",
    "            if verbose: print(f\"{scorer.__name__} ile sonuç bulunamadı.\")\n",
    "\n",
    "    if filtered_df.is_empty():\n",
    "        if verbose: print(\"Tüm denemelere rağmen eşleşme bulunamadı. NER sonucuyla fallback yapılıyor.\")\n",
    "        fallback_data = {}\n",
    "        # Not: Fallback için en son denemedeki `found_...` değişkenlerini kullanıyoruz.\n",
    "        found_entities = {\"il\": found_il, \"ilce\": found_ilce, \"mahalle\": found_mahalle, \"sokak\": found_sokak}\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            fallback_data[df_col_name] = found_entities.get(dict_key)\n",
    "            fallback_data[f'{df_col_name}_score'] = -1.0\n",
    "        fallback_data['Overall_Score'] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "        \n",
    "  \n",
    "    score_expressions = []\n",
    "    score_columns_for_avg = []\n",
    "    for dict_key, df_col_name in mapping.items():\n",
    "        text_value = entity_dict.get(dict_key)\n",
    "        score_col_name = f'{df_col_name}_score'\n",
    "        score_columns_for_avg.append(score_col_name)\n",
    "        if text_value:\n",
    "            choices = filtered_df.get_column(df_col_name).cast(pl.String).to_list()\n",
    "            scores = process.cdist([text_value], choices, scorer=successful_scorer, score_cutoff=0)[0]\n",
    "            score_expressions.append(pl.Series(name=score_col_name, values=scores))\n",
    "        else:\n",
    "            score_expressions.append(pl.lit(0).alias(score_col_name))\n",
    "            \n",
    "            \n",
    "    final_df = filtered_df.with_columns(score_expressions)\n",
    "\n",
    "    if score_columns_for_avg:\n",
    "        final_df = final_df.with_columns(pl.mean_horizontal(score_columns_for_avg).alias(\"Overall_Score\"))\n",
    "    else:\n",
    "        final_df = final_df.with_columns(pl.lit(0).alias(\"Overall_Score\"))\n",
    "        \n",
    "    final_df = final_df.sort(\"Overall_Score\", descending=True)\n",
    "        \n",
    "    if return_top > 0:\n",
    "        return final_df.head(return_top)\n",
    "    else:\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe216729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter # Oylama için Counter kullanacağız\n",
    "\n",
    "def calculate_match_scores_all_scorers(df: pl.DataFrame,\n",
    "                                 entity_dict: dict,\n",
    "                                 mapping: dict,\n",
    "                                 score_cutoff = 85,\n",
    "                                 return_top: int = 0,\n",
    "                                 verbose: int = 0,\n",
    "                                 ) -> pl.DataFrame:\n",
    "    \n",
    "    scorers_to_try = [\n",
    "        fuzz.token_set_ratio,       \n",
    "        fuzz.ratio, \n",
    "        fuzz.token_sort_ratio,     \n",
    "    ]\n",
    "    \n",
    "    # --- AŞAMA 1: BİLGİ TOPLAMA ---\n",
    "    # Her scorer'ın her varlık için ne bulduğunu bu sözlükte toplayalım.\n",
    "    found_entities = {\"il\": [], \"ilce\": [], \"mahalle\": [], \"sokak\": []}\n",
    "\n",
    "    for scorer in scorers_to_try:\n",
    "        il = find_trusted_entity_cached(entity_dict.get(\"il\"), ALL_CITIES, \"il\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        ilce = find_trusted_entity_cached(entity_dict.get(\"ilce\"), ALL_DISTRICTS, \"ilce\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        mahalle = find_trusted_entity_cached(entity_dict.get(\"mahalle\"), ALL_QUARTERS, \"mahalle\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        sokak, _ = find_best_street_match(entity_dict.get(\"sokak\"), scorer=scorer, score_cutoff=score_cutoff + 5)\n",
    "        \n",
    "        if il: found_entities[\"il\"].append(il)\n",
    "        if ilce: found_entities[\"ilce\"].append(ilce)\n",
    "        if mahalle: found_entities[\"mahalle\"].append(mahalle)\n",
    "        if sokak: found_entities[\"sokak\"].append(sokak)\n",
    "    \n",
    "    # --- AŞAMA 2: KAPSAYICI FİLTRELEME (YENİ MANTIK) ---\n",
    "    # Her varlık için bulunan tüm benzersiz adayları kullanarak filtreleme yap.\n",
    "    filtered_df = df\n",
    "    \n",
    "    # entity_dict'te var olan ve en az bir scorer tarafından bulunan varlıkları filtrele\n",
    "    for dict_key, df_col_name in mapping.items():\n",
    "        found_list = found_entities.get(dict_key)\n",
    "        \n",
    "        if entity_dict.get(dict_key) and found_list:\n",
    "            # Bulunan varlıkları tekilleştir. Örn: ['bagarasi', 'yenibagarasi', 'bagarasi'] -> {'bagarasi', 'yenibagarasi'}\n",
    "            unique_entities = list(set(found_list))\n",
    "            \n",
    "            if verbose: print(f\"Filtreye ekleniyor -> {df_col_name}.is_in({unique_entities})\")\n",
    "            \n",
    "            # Polars'ın .is_in() fonksiyonu ile 'OR' mantığında filtreleme yap\n",
    "            filtered_df = filtered_df.filter(pl.col(df_col_name).is_in(unique_entities))\n",
    "            \n",
    "    # --- AŞAMA 3: SKORLAMA VE SONUÇLANDIRMA ---\n",
    "    if filtered_df.is_empty():\n",
    "        if verbose: print(\"Kapsayıcı filtrelemeye rağmen eşleşme bulunamadı. Fallback yapılıyor.\")\n",
    "        \n",
    "        # HATA DÜZELTMESİ: Fallback mantığını doğru şekilde implement et\n",
    "        fallback_data = {}\n",
    "        # En çok oy alan sonucu (veya herhangi birini) fallback için kullanabiliriz\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            found_list = found_entities.get(dict_key)\n",
    "            fallback_value = Counter(found_list).most_common(1)[0][0] if found_list else None\n",
    "            fallback_data[df_col_name] = fallback_value\n",
    "            fallback_data[f'{df_col_name}_score'] = -1.0\n",
    "        fallback_data['Overall_Score'] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "\n",
    "    # Skorlama mantığı (öncekiyle aynı, en doğru hali)\n",
    "    if verbose: print(f\"\\nFiltreleme tamamlandı. {filtered_df.height} aday üzerinden final skorları hesaplanıyor...\")\n",
    "    \n",
    "    score_expressions = []\n",
    "    score_columns_for_avg = []\n",
    "    for dict_key, df_col_name in mapping.items():\n",
    "        text_value = entity_dict.get(dict_key)\n",
    "        score_col_name = f'{df_col_name}_score'\n",
    "        score_columns_for_avg.append(score_col_name)\n",
    "        if text_value and df_col_name in filtered_df.columns:\n",
    "            choices = filtered_df.get_column(df_col_name).cast(pl.String).to_list()\n",
    "            all_scores = [process.cdist([text_value], choices, scorer=s)[0] for s in scorers_to_try]\n",
    "            best_scores = np.max(np.array(all_scores), axis=0)\n",
    "            score_expressions.append(pl.Series(name=score_col_name, values=best_scores))\n",
    "        else:\n",
    "            score_expressions.append(pl.lit(0).alias(score_col_name))\n",
    "\n",
    "    final_df = filtered_df.with_columns(score_expressions)\n",
    "\n",
    "    if score_columns_for_avg:\n",
    "        final_df = final_df.with_columns(pl.mean_horizontal(score_columns_for_avg).alias(\"Overall_Score\"))\n",
    "    else:\n",
    "        final_df = final_df.with_columns(pl.lit(0).alias(\"Overall_Score\"))\n",
    "        \n",
    "    final_df = final_df.sort(\"Overall_Score\", descending=True)\n",
    "        \n",
    "    return final_df.head(return_top) if return_top > 0 else final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99e97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_scores_hypothesis(df: pl.DataFrame,\n",
    "                                     entity_dict: dict,\n",
    "                                     mapping: dict,\n",
    "                                     score_cutoff = 85,\n",
    "                                     return_top: int = 0,\n",
    "                                     verbose: int = 0,\n",
    "                                     ) -> pl.DataFrame:\n",
    "    \n",
    "    scorers_to_try = [\n",
    "        fuzz.token_set_ratio,       \n",
    "        fuzz.ratio, \n",
    "        fuzz.token_sort_ratio,     \n",
    "    ]\n",
    "    \n",
    "    # --- AŞAMA 1: HİPOTEZLERİ OLUŞTURMA ---\n",
    "    hypotheses = []\n",
    "    consolidated_found_entities = {\"il\": [], \"ilce\": [], \"mahalle\": [], \"sokak\": []}\n",
    "    \n",
    "    if verbose: print(\"--- AŞAMA 1: Her scorer için hipotezler oluşturuluyor... ---\")\n",
    "    for scorer in scorers_to_try:\n",
    "        il = find_trusted_entity_cached(entity_dict.get(\"il\"), ALL_CITIES, \"il\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        ilce = find_trusted_entity_cached(entity_dict.get(\"ilce\"), ALL_DISTRICTS, \"ilce\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        mahalle = find_trusted_entity_cached(entity_dict.get(\"mahalle\"), ALL_QUARTERS, \"mahalle\", scorer=scorer, trust_threshold=score_cutoff)\n",
    "        #sokak = find_trusted_entity_cached(entity_dict.get(\"sokak\"),ALL_STREETS,\"sokak\",scorer=scorer, trust_threshold=score_cutoff)\n",
    "        sokak, _ = find_best_street_match(entity_dict.get(\"sokak\"), scorer=scorer, score_cutoff=score_cutoff + 5)\n",
    "        \n",
    "        hypothesis = {\"scorer_name\": scorer.__name__, \"il\": il, \"ilce\": ilce, \"mahalle\": mahalle, \"sokak\": sokak}\n",
    "        \n",
    "        if any(v for k, v in hypothesis.items() if k != 'scorer_name'):\n",
    "            hypotheses.append(hypothesis)\n",
    "            if il: consolidated_found_entities[\"il\"].append(il)\n",
    "            if ilce: consolidated_found_entities[\"ilce\"].append(ilce)\n",
    "            if mahalle: consolidated_found_entities[\"mahalle\"].append(mahalle)\n",
    "            if sokak: consolidated_found_entities[\"sokak\"].append(sokak)\n",
    "            if verbose > 1: print(f\"  -> {scorer.__name__} hipotezi: { {k:v for k,v in hypothesis.items() if v} }\")\n",
    "\n",
    "    if not hypotheses:\n",
    "        if verbose: print(\"Hiçbir scorer anlamlı bir hipotez üretemedi. Fallback yapılıyor.\")\n",
    "          # --- DÜZELTME 1: İLK FALLBACK MEKANİZMASI ---\n",
    "        # Sütun sırasını garantilemek için önce isimleri, sonra skorları ekliyoruz.\n",
    "        fallback_data = {}\n",
    "        for col_name in mapping.values():\n",
    "            fallback_data[col_name] = None\n",
    "        for col_name in mapping.values():\n",
    "            fallback_data[f\"{col_name}_score\"] = -1.0 # Skorları -1 olarak ayarlayalım\n",
    "        \n",
    "        fallback_data['Overall_Score'] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "            \n",
    "\n",
    "    # --- AŞAMA 2: HER HİPOTEZİ AYRI AYRI DEĞERLENDİRME ---\n",
    "    best_result_from_all_hypotheses = pl.DataFrame()\n",
    "    if verbose: print(\"\\n--- AŞAMA 2: Her hipotez ayrı ayrı test ediliyor... ---\")\n",
    "    for hypo in hypotheses:\n",
    "        if verbose > 1: print(f\"-> Test edilen hipotez ({hypo['scorer_name']}):\")\n",
    "        \n",
    "        temp_df = df\n",
    "        if hypo.get(\"il\"): temp_df = temp_df.filter(pl.col(\"city_name\") == hypo[\"il\"])\n",
    "        if hypo.get(\"ilce\"): temp_df = temp_df.filter(pl.col(\"district_name\") == hypo[\"ilce\"])\n",
    "        if hypo.get(\"mahalle\"): temp_df = temp_df.filter(pl.col(\"quarter_name\") == hypo[\"mahalle\"])\n",
    "        if hypo.get(\"sokak\"): temp_df = temp_df.filter(pl.col(\"street_name\") == hypo[\"sokak\"])\n",
    "        \n",
    "        if temp_df.is_empty():\n",
    "            if verbose > 1: print(f\"  Bu hipotez ile aday bulunamadı.\")\n",
    "            continue\n",
    "            \n",
    "        score_expressions = []\n",
    "        main_score_columns = []\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            text_value = entity_dict.get(dict_key)\n",
    "            score_col_name = f'{df_col_name}_score'\n",
    "            main_score_columns.append(score_col_name)\n",
    "            \n",
    "            if text_value and df_col_name in temp_df.columns:\n",
    "                choices = temp_df.get_column(df_col_name).cast(pl.String).to_list()\n",
    "                all_scores = [process.cdist([text_value], choices, scorer=s)[0] for s in scorers_to_try]\n",
    "                best_scores = np.max(np.array(all_scores), axis=0)\n",
    "                score_expressions.append(pl.Series(name=score_col_name, values=best_scores))\n",
    "            else:\n",
    "                score_expressions.append(pl.lit(0).alias(score_col_name))\n",
    "        \n",
    "        hypothesis_result_df = temp_df.with_columns(score_expressions).with_columns(\n",
    "            pl.mean_horizontal(main_score_columns).alias(\"Overall_Score\")\n",
    "        ).sort(\"Overall_Score\", descending=True).head(1)\n",
    "        \n",
    "        if verbose: print(f\"  -> {hypo['scorer_name']} hipotezinin en iyi sonucu: {hypothesis_result_df.get_column('Overall_Score')[0]:.2f} skor\")\n",
    "        \n",
    "        best_result_from_all_hypotheses = pl.concat([\n",
    "            best_result_from_all_hypotheses,\n",
    "            hypothesis_result_df\n",
    "        ])\n",
    "\n",
    "    # --- AŞAMA 3: EN İYİ HİPOTEZİN SONUCUNU SEÇME ---\n",
    "    if best_result_from_all_hypotheses.is_empty():\n",
    "        if verbose: print(\"\\nTüm hipotezler test edildi ancak veritabanında eşleşme bulunamadı. En iyi NER tahminiyle fallback yapılıyor.\")\n",
    "        fallback_data = {}\n",
    "        # 1. Adım: Önce sadece isim sütunlarını doldur\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            found_list = consolidated_found_entities.get(dict_key)\n",
    "            fallback_value = Counter(found_list).most_common(1)[0][0] if found_list else None\n",
    "            fallback_data[df_col_name] = fallback_value\n",
    "        \n",
    "        # 2. Adım: Sonra skor sütunlarını ekle\n",
    "        for dict_key, df_col_name in mapping.items():\n",
    "            fallback_data[f'{df_col_name}_score'] = -1.0\n",
    "            \n",
    "        fallback_data['Overall_Score'] = -1.0\n",
    "        return pl.DataFrame([fallback_data])\n",
    "        \n",
    "    if verbose: print(\"\\n--- AŞAMA 3: Tüm hipotezler arasından en iyi sonuç seçiliyor. ---\")\n",
    "    \n",
    "    final_df = best_result_from_all_hypotheses.sort(\"Overall_Score\", descending=True)\n",
    "        \n",
    "    if return_top > 0:\n",
    "        return final_df.head(return_top)\n",
    "    else:\n",
    "        return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acedae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c6cbc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>city_name</th><th>district_name</th><th>quarter_name</th><th>street_name</th><th>city_name_score</th><th>district_name_score</th><th>quarter_name_score</th><th>street_name_score</th><th>Overall_Score</th></tr><tr><td>cat</td><td>cat</td><td>cat</td><td>str</td><td>f32</td><td>i32</td><td>i32</td><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mugla&quot;</td><td>&quot;milas&quot;</td><td>&quot;haci ilyas&quot;</td><td>&quot;avcilar&quot;</td><td>100.0</td><td>0</td><td>0</td><td>100.0</td><td>50.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ city_name ┆ district_ ┆ quarter_n ┆ street_na ┆ … ┆ district_ ┆ quarter_n ┆ street_na ┆ Overall_ │\n",
       "│ ---       ┆ name      ┆ ame       ┆ me        ┆   ┆ name_scor ┆ ame_score ┆ me_score  ┆ Score    │\n",
       "│ cat       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ e         ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ cat       ┆ cat       ┆ str       ┆   ┆ ---       ┆ i32       ┆ f32       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ i32       ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ mugla     ┆ milas     ┆ haci      ┆ avcilar   ┆ … ┆ 0         ┆ 0         ┆ 100.0     ┆ 50.0     │\n",
       "│           ┆           ┆ ilyas     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"MUĞLA 48 REZİDANS AVCILAR SOK NO 5  DAİRE  329\"\n",
    "entity_dict = ent2dict(nlp(query))\n",
    "calculate_match_scores_hypothesis(base_df,entity_dict,key_to_column_map,return_top=1,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87b90254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>city_name</th><th>district_name</th><th>quarter_name</th><th>street_name</th><th>city_name_score</th><th>district_name_score</th><th>quarter_name_score</th><th>street_name_score</th><th>Overall_Score</th></tr><tr><td>cat</td><td>cat</td><td>cat</td><td>str</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;denizli&quot;</td><td>&quot;saraykoy&quot;</td><td>&quot;turan&quot;</td><td>&quot;zahire pazari&quot;</td><td>0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>75.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ city_name ┆ district_ ┆ quarter_n ┆ street_na ┆ … ┆ district_ ┆ quarter_n ┆ street_na ┆ Overall_ │\n",
       "│ ---       ┆ name      ┆ ame       ┆ me        ┆   ┆ name_scor ┆ ame_score ┆ me_score  ┆ Score    │\n",
       "│ cat       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ e         ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ cat       ┆ cat       ┆ str       ┆   ┆ ---       ┆ f32       ┆ f32       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ f32       ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ denizli   ┆ saraykoy  ┆ turan     ┆ zahire    ┆ … ┆ 100.0     ┆ 100.0     ┆ 100.0     ┆ 75.0     │\n",
       "│           ┆           ┆           ┆ pazari    ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dict = ent2dict(nlp(query))\n",
    "calculate_match_scores(base_df,entity_dict,key_to_column_map,return_top=1,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df94da55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>city_name</th><th>district_name</th><th>quarter_name</th><th>street_name</th><th>city_name_score</th><th>district_name_score</th><th>quarter_name_score</th><th>street_name_score</th><th>Overall_Score</th></tr><tr><td>cat</td><td>cat</td><td>cat</td><td>str</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;denizli&quot;</td><td>&quot;saraykoy&quot;</td><td>&quot;turan&quot;</td><td>&quot;zahire pazari&quot;</td><td>0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>75.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ city_name ┆ district_ ┆ quarter_n ┆ street_na ┆ … ┆ district_ ┆ quarter_n ┆ street_na ┆ Overall_ │\n",
       "│ ---       ┆ name      ┆ ame       ┆ me        ┆   ┆ name_scor ┆ ame_score ┆ me_score  ┆ Score    │\n",
       "│ cat       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ e         ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ cat       ┆ cat       ┆ str       ┆   ┆ ---       ┆ f32       ┆ f32       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ f32       ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ denizli   ┆ saraykoy  ┆ turan     ┆ zahire    ┆ … ┆ 100.0     ┆ 100.0     ┆ 100.0     ┆ 75.0     │\n",
       "│           ┆           ┆           ┆ pazari    ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dict = ent2dict(nlp(query))\n",
    "calculate_match_scores_all_scorers(base_df,entity_dict,key_to_column_map,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe660496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "test_df = pl.read_csv(\"../dataset/test.csv\")\n",
    "# --- 1. AYARLAR ---\n",
    "BATCH_SIZE = 50_000\n",
    "OUTPUT_DIRECTORY = \"featured_test_parquet\" \n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "results_list = []\n",
    "batch_counter = 0  # <-- 3. ÖNERİ: Temiz bir sayaç başlattık\n",
    "\n",
    "try:\n",
    "    for i, row in enumerate(tqdm(test_df.iter_rows(named=True), total=test_df.height, desc=\"Eğitim Seti İşleniyor\")):\n",
    "        \n",
    "        # --- Ana işlem mantığı (değişiklik yok) ---\n",
    "        address_text = row['address']\n",
    "        doc = nlp(address_text)\n",
    "        ent_dict = ent2dict(doc)\n",
    "    \n",
    "        best_match_df = calculate_match_scores_hypothesis(\n",
    "            df=base_df,\n",
    "            entity_dict=ent_dict,\n",
    "            mapping=key_to_column_map,\n",
    "            return_top=1\n",
    "        )\n",
    "        \n",
    "        result_row = row\n",
    "        if not best_match_df.is_empty():\n",
    "            match_dict = best_match_df.to_dicts()[0]\n",
    "            result_row.update(match_dict)\n",
    "        \n",
    "        if 'address' in result_row and isinstance(result_row['address'], str):\n",
    "            result_row['address'] = result_row['address'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "        results_list.append(result_row)\n",
    "        \n",
    "        # --- Batch yazma mantığı ---\n",
    "        # <-- 1. ÖNERİ: Batch koşulu (i + 1) olarak güncellendi\n",
    "        if (i + 1) % BATCH_SIZE == 0:\n",
    "            batch_counter += 1 # <-- 3. ÖNERİ: Sayacı artır\n",
    "            print(f\"\\nBatch {batch_counter} diske yazılıyor...\")\n",
    "            \n",
    "            batch_df = pl.DataFrame(results_list)\n",
    "            output_path = f\"{OUTPUT_DIRECTORY}/batch_{batch_counter}.parquet\"\n",
    "            batch_df.write_parquet(output_path)\n",
    "            \n",
    "            results_list = [] # Listeyi sıfırla\n",
    "            print(f\"Batch yazıldı ve bellek temizlendi.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nİşlem kullanıcı tarafından durduruldu. Kalan sonuçlar kaydediliyor.\")\n",
    "    # pass komutu sayesinde kod finally bloğuna devam edecek\n",
    "\n",
    "finally:\n",
    "    # <-- 2. ÖNERİ: Son kaydetme işlemini finally bloğuna aldık\n",
    "    if results_list:\n",
    "        batch_counter += 1 # <-- 3. ÖNERİ: Son batch için de sayacı artır\n",
    "        print(f\"\\nDöngü bitti/durduruldu. Kalan {len(results_list)} satırlık son batch ({batch_counter}) diske yazılıyor...\")\n",
    "        \n",
    "        batch_df = pl.DataFrame(results_list)\n",
    "        output_path = f\"{OUTPUT_DIRECTORY}/batch_{batch_counter}.parquet\"\n",
    "        batch_df.write_parquet(output_path)\n",
    "        \n",
    "        results_list = []\n",
    "        print(f\"Son batch yazıldı ve bellek temizlendi.\")\n",
    "\n",
    "    print(\"\\nTüm işlemler tamamlandı.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
